## Reflection on AI’s Role in Building RainCheck (≈500 words)

Working with AI as a pair‑programmer on RainCheck fundamentally changed my development cadence. The most noticeable improvement was momentum: scaffolding a React + Vite project, setting up Tailwind + shadcn/ui, wiring common utilities, and generating consistent TypeScript docstrings happened quickly. That acceleration created more time to focus on product‑level decisions like event flows, user feedback during loading, and the shape of domain types.

What worked well: conversational iteration and structured edits. I could describe intent (“wire SearchBox to current weather and show loading/error”) and the AI translated that into multi‑file changes with a coherent event model. The use of an event bus (`weather:current-updated`, `weather:error`, `weather:loading`, `weather:location-selected`) emerged naturally as we iterated, keeping components decoupled while still synchronized. Another strong point was boilerplate elimination: shadcn/ui components, Tailwind config, and Recharts setup are all common but time‑consuming; having those created and correctly imported let me validate UX rapidly. The AI also did well at enforcing naming and typing conventions (PascalCase components, camelCase utilities, strict interfaces) and at producing docstrings that captured both “what” and “why,” which is often skipped when working quickly.

Where it felt limiting: environmental context and external service nuances. For example, ensuring all environment variables were standardized to `import.meta.env` (Vite) required explicit correction after initial drafts mixed `process.env`. Similarly, while the AI is strong at writing HTTP requests, I still needed to confirm endpoint variants (OpenWeather Geo API vs Current/Forecast) and make decisions about mapping API responses to internal types. The AI can produce a reasonable mapping, but validating those assumptions (e.g., units, icon formats, visibility units in km) remains my job. Another limitation is intentional state management: using a lightweight custom event bus is great for early iterations, but I still have to evaluate if that should evolve into a more formal state pattern as features grow (e.g., query caching, pagination, or persistence‑driven UI).

Prompting, reviewing, and iterating: I learned to be unambiguous and contextual. The best results came when prompts included concrete constraints (framework, router, env style, file paths) and when I requested “apply changes directly to files” rather than just receiving code snippets. I also found that asking for docstrings up front led to better API surfaces—types improved because the AI had to articulate purpose and return shapes. Reviewing changes benefited from brief summaries paired with targeted diffs; this reduced the overhead of validating multi‑file edits. When something was off, short corrective prompts (“standardize env to import.meta.env only”, “fetch by lat/lon for suggestions”) produced accurate follow‑ups.

What I learned: AI is excellent at accelerating scaffolding, repetitive patterns, and first‑pass integrations. It is strongest when I define constraints and outcomes clearly, then keep a tight review loop. The code quality holds up well if I treat the AI like a junior engineer who works fast but needs clear direction and code review. The AI shines with tasks that benefit from consistency—types, docstrings, folder structure—where human developers might drift under time pressure. Conversely, I still own architectural judgment, edge‑case handling, and evolving state patterns. In short, AI amplified my output, but my role remained crucial: decide, constrain, verify, and iterate.


